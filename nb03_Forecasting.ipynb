{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e6fe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "\n",
    "import ipywidgets as widgets\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List\n",
    "\n",
    "from ipywidgets import interact, interact_manual\n",
    "\n",
    "\n",
    "sns.set_style()\n",
    "\n",
    "LAGGED_SALES_DATA = 'lagged_sales_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebd382c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1107 entries, 0 to 1106\n",
      "Data columns (total 28 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   year                   1107 non-null   int64  \n",
      " 1   week                   1107 non-null   int64  \n",
      " 2   article_count          1107 non-null   float64\n",
      " 3   sales_price            1107 non-null   float64\n",
      " 4   original_price         1107 non-null   float64\n",
      " 5   discount               1107 non-null   float64\n",
      " 6   stock_begin_week       1107 non-null   float64\n",
      " 7   month                  1107 non-null   int64  \n",
      " 8   transaction_date       1107 non-null   object \n",
      " 9   article_count_minus-1  1107 non-null   float64\n",
      " 10  article_count_minus-2  1107 non-null   float64\n",
      " 11  article_count_minus-3  1107 non-null   float64\n",
      " 12  article_count_minus-4  1107 non-null   float64\n",
      " 13  article_7754922460402  1107 non-null   int64  \n",
      " 14  article_7754922460403  1107 non-null   int64  \n",
      " 15  article_7754922460404  1107 non-null   int64  \n",
      " 16  article_7754922460405  1107 non-null   int64  \n",
      " 17  article_7754922460406  1107 non-null   int64  \n",
      " 18  article_7754922460407  1107 non-null   int64  \n",
      " 19  article_7754922710402  1107 non-null   int64  \n",
      " 20  article_7754922710403  1107 non-null   int64  \n",
      " 21  article_7754922710404  1107 non-null   int64  \n",
      " 22  article_7754922710405  1107 non-null   int64  \n",
      " 23  article_7754922710406  1107 non-null   int64  \n",
      " 24  article_7754922710407  1107 non-null   int64  \n",
      " 25  country_country_0      1107 non-null   int64  \n",
      " 26  country_country_1      1107 non-null   int64  \n",
      " 27  country_country_2      1107 non-null   int64  \n",
      "dtypes: float64(9), int64(18), object(1)\n",
      "memory usage: 242.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df_final = pd.read_csv(LAGGED_SALES_DATA)\n",
    "df_final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc0387",
   "metadata": {},
   "source": [
    "## Start and End Dates of Train-Test Period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bd2da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = '2019-10-01'\n",
    "END_DATE = '2020-03-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6285ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dates = list(df_final.transaction_date.unique())\n",
    "date_range = sorted([dt for dt in list_of_dates if dt >= START_DATE and dt <= END_DATE])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe31a5a",
   "metadata": {},
   "source": [
    "### Check Date Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0f18e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dt in date_range:\n",
    "    extract = df_final[df_final.transaction_date == dt]\n",
    "    if len(extract) == 0:\n",
    "        print(f'No records for date = {dt}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ff5b0f",
   "metadata": {},
   "source": [
    "## Train-Test\n",
    "\n",
    "## Rolling Window Forecasts\n",
    "\n",
    "In what follows, we use the data from October 2019 till March 2020 and estimate the goodness of the model with the predictors as stated below. In this context, for each week in the `date_range`, we train the model for data that appear before this week and predict the article count for this week. We keep records of the week, the actual and the forecasted values. As we move ahead in time, the model sees more and more of the data but the forecasts are always one week ahead of the last date in the training data. This is how one would use such models in reality. They would be trained on all the data that is available up until that point of time and forecast demand, naturally, ahead of time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a8a365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE = ['article_count']\n",
    "PREDICTORS = ['year', \n",
    "              'week', \n",
    "              'month', \n",
    "              'original_price',\n",
    "              'article_count_minus-1', \n",
    "              'country_country_0', \n",
    "              'country_country_1', \n",
    "              'country_country_2',\n",
    "              'article_7754922460402',\n",
    "              'article_7754922460403',\n",
    "              'article_7754922460404',\n",
    "              'article_7754922460405',\n",
    "              'article_7754922460406',\n",
    "              'article_7754922460407',\n",
    "              'article_7754922710402',\n",
    "              'article_7754922710403',\n",
    "              'article_7754922710404',\n",
    "              'article_7754922710405',\n",
    "              'article_7754922710406',\n",
    "              'article_7754922710407']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbff7e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "# (article_id, country) -> {transaction_date, actual, predicted}}\n",
    "article_prediction_dict = {}\n",
    "\n",
    "articles = ['article_7754922460402',\n",
    "            'article_7754922460403',\n",
    "            'article_7754922460404',\n",
    "            'article_7754922460405',\n",
    "            'article_7754922460406',\n",
    "            'article_7754922460407',\n",
    "            'article_7754922710402',\n",
    "            'article_7754922710403',\n",
    "            'article_7754922710404',\n",
    "            'article_7754922710405',\n",
    "            'article_7754922710406',\n",
    "            'article_7754922710407']\n",
    "\n",
    "countries = ['country_country_0', \n",
    "             'country_country_1', \n",
    "             'country_country_2']\n",
    "\n",
    "for dt in date_range:\n",
    "    train_data = df_final[df_final.transaction_date < dt]\n",
    "    test_data = df_final[df_final.transaction_date == dt]\n",
    "    \n",
    "    X_train, y_train = train_data[PREDICTORS], train_data[RESPONSE]\n",
    "   \n",
    "    model = XGBRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    for article, country in product(articles, countries):\n",
    "        # Get test data for (article, country) combination\n",
    "        cond = (test_data[article] == 1) & (test_data[country] == 1)\n",
    "        X_test = test_data[cond][PREDICTORS].copy()\n",
    "        y_test = test_data[cond][RESPONSE].copy()\n",
    "        \n",
    "        actual = np.nan\n",
    "        predicted = np.nan \n",
    "        y_pred = np.array([])\n",
    "        \n",
    "        # Predict for (article, country) combination\n",
    "        if len(X_test) > 0:\n",
    "            y_pred = model.predict(X_test)\n",
    "            predicted = round(y_pred[0], 0)\n",
    "            actual = y_test.values[0][0]\n",
    "        \n",
    "        # Record actuals and predicted for (article, country) combination\n",
    "        article_prediction_dict.setdefault((article, country), []).append({'transaction_date': dt, \n",
    "                                                                           'actual': actual, \n",
    "                                                                           'predicted': predicted})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9184c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac2ca9357414639a7316b73c20434de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='article', options=('article_7754922460402', 'article_7754922460403â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def plot_actuals_and_predicted(article=articles, country=countries):\n",
    "    # Extract data for (article, country)\n",
    "    art_c_data = article_prediction_dict[(article, country)]\n",
    "    \n",
    "    # Sort the data by transaction date\n",
    "    sorted_data = sorted(art_c_data, key = lambda x: x['transaction_date'])\n",
    "    \n",
    "    # Extract the values\n",
    "    x_vals = [entry['transaction_date'] for entry in sorted_data]\n",
    "    actual_vals = [entry['actual'] for entry in sorted_data]\n",
    "    predicted_vals = [entry['predicted'] for entry in sorted_data]\n",
    "    \n",
    "    mean_mae = np.nan\n",
    "    if len(actual_vals) > 0:\n",
    "        mean_mae = sum([abs(a - p) for a, p in zip(actual_vals, predicted_vals)]) / len(actual_vals)\n",
    "\n",
    "    \n",
    "    # Plot!\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 7))\n",
    "    ax.plot(x_vals, actual_vals, label='Actual')\n",
    "    ax.plot(x_vals, predicted_vals, label='Predicted')\n",
    "    \n",
    "    ax.set_xlabel('transaction_date')\n",
    "    \n",
    "    plt.title(f'Mean MAE = {mean_mae}')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "119defac",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_data = df_final[(df_final.transaction_date >= START_DATE) & \n",
    "                         (df_final.transaction_date <= END_DATE)].copy()\n",
    "\n",
    "\n",
    "def abs_error(row):\n",
    "    return abs(row['article_count'] - row['article_count_minus-1'])\n",
    "\n",
    "relevant_data.loc[:, 'abs_error'] = relevant_data.apply(lambda row: abs_error(row), axis=1)\n",
    "\n",
    "group_by_cols = articles + countries\n",
    "\n",
    "relevant_data.loc[:, 'mean_abs_error'] = relevant_data.groupby(group_by_cols)['abs_error'].transform(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e31e8c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5fb3359461b4342bb8425d48b44eb9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='article', options=('article_7754922460402', 'article_7754922460403â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact\n",
    "def get_baseline_error(article=articles, country=countries):\n",
    "    # Extract data for (article, country)\n",
    "    mean_mae =relevant_data[(relevant_data[article] == 1) & \n",
    "                            (relevant_data[country] == 1)]['mean_abs_error'].unique()\n",
    "    if len(mean_mae) > 0:\n",
    "        return mean_mae[0]\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68b55e1",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The baseline forecaster has a lower mean MAE than the XGBoost forecaster. Of course, no effort was made to tune hyperparameters. But this shows that for articles that sell sparingly, even a \"dumb\" forecaster will do a good job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ffce188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipywidgets: 7.6.3\n",
      "numpy     : 1.20.2\n",
      "seaborn   : 0.11.1\n",
      "pandas    : 1.2.4\n",
      "matplotlib: 3.4.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark --iversions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
